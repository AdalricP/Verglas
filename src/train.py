# -*- coding: utf-8 -*-
"""Train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Pav41-zGGHSLczY_XgekxOs0o9DubuNc
"""

print("Runtime Started...")

import os
import torch
from lxml import etree
from torch.utils.data import Dataset, DataLoader
from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW
from torch.cuda.amp import GradScaler, autocast

print("Imported Required Packages...")

class MusicXMLDataset(Dataset):
    def __init__(self, data_dir, tokenizer, max_length=512):
        """
        Initializes the MusicXML dataset.
        """
        self.data_dir = data_dir
        self.tokenizer = tokenizer
        self.max_length = max_length

        # Collect all MusicXML files with .musicxml extension
        self.files = [
            os.path.join(data_dir, f) for f in os.listdir(data_dir)
            if f.lower().endswith(".musicxml")
        ]

        if not self.files:
            raise ValueError(f"No MusicXML files found in directory: {data_dir}")

    def __len__(self):
        return len(self.files)

    def __getitem__(self, idx):
        file_path = self.files[idx]

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                music_tokens = f.read()
        except Exception as e:
            print(f"Error reading {file_path}: {e}")
            raise e

        # Tokenize the text representation
        inputs = self.tokenizer.encode(
            music_tokens,
            truncation=True,
            max_length=self.max_length,
            padding="max_length"
        )

        labels = inputs.copy()
        return torch.tensor(inputs, dtype=torch.long), torch.tensor(labels, dtype=torch.long)

def validate_musicxml(xml_string):
    """
    Validates the syntax of a MusicXML string.
    """
    try:
        # Parse XML
        root = etree.fromstring(xml_string)
        # Additional checks can be added here (e.g., required tags)
        return True, "Valid MusicXML"
    except etree.XMLSyntaxError as e:
        return False, str(e)

# Collate Function for DataLoader
def collate_fn(batch):
    inputs, labels = zip(*batch)
    inputs = torch.stack(inputs)
    labels = torch.stack(labels)
    return inputs, labels

# Step 2: Load Tokenizer
# Use GPT2Tokenizer and add custom MusicXML tokens if needed
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
tokenizer.add_special_tokens({"pad_token": "<|pad|>", "eos_token": "<|endofmusic|>", "bos_token": "<|startofmusic|>"})

# Step 4: Model Setup
model = GPT2LMHeadModel.from_pretrained('gpt2')
model.resize_token_embeddings(len(tokenizer))
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/Verglas_Project_Files

# Step 3: Dataset and DataLoader
music_data_dir = f"{os.getcwd()}/LeiderCorpusMusicXMLSample"  # Replace with your dataset path
max_length = 512

# Training Loop
epochs = 3
learning_rate = 5e-5
batch_size = 1  # Reduce batch size to fit memory
accumulation_steps = 4  # Simulates larger batch sizes
scaler = GradScaler()  # For mixed precision
structure_error_weight = 1 # Increase to prioritize structure while learning.

data_loader = DataLoader(
    MusicXMLDataset(music_data_dir, tokenizer=tokenizer),
    batch_size=batch_size,
    collate_fn=collate_fn,
    pin_memory=True  # Speeds up data transfer
)

optimizer = AdamW(model.parameters(), lr=learning_rate)

for epoch in range(epochs):
    model.train()
    epoch_loss = 0
    for step, (inputs, labels) in enumerate(data_loader):
        inputs, labels = inputs.to(device), labels.to(device)

        with autocast():  # Mixed precistion training
            output = model(inputs, labels=labels)
            base_loss = output.loss
            logits = output.logits

            predicted_tokens = torch.argmax(logits, dim=-1)  # Get the token with the highest score
            predicted_texts = [tokenizer.decode(tokens, skip_special_tokens=True) for tokens in predicted_tokens]

            penalties = torch.zeros(len(predicted_texts)).to(device)  # Initialize penalties
            for i, text in enumerate(predicted_texts):  # Iterate over batch outputs
                if not validate_musicxml(text):  # Check if the output is invalid
                    penalties[i] = structure_error_weight  # Apply penalty for invalid output

            total_loss = base_loss + penalties.mean()

        total_loss = total_loss / accumulation_steps
        scaler.scale(total_loss).backward()  # Scaled loss for mixed precision

        if (step + 1) % accumulation_steps == 0 or (step + 1) == len(data_loader):
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()

        epoch_loss += total_loss.item() * accumulation_steps  # Scale loss back for logging


        if step % 10 == 0:
            print(f"Epoch: {epoch}, Step: {step}, Loss: {(epoch_loss/ len(data_loader)):.4f}")

    print(f"Epoch {epoch} finished with loss {epoch_loss / len(data_loader):.4f}")

    # Save checkpoint
    torch.save(model.state_dict(), f"model_epoch_{epoch}.pth")

# Step 6: Save the Fine-Tuned Model9
output_dir = "./fine_tuned_music_model"
os.makedirs(output_dir, exist_ok=True)
model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)

print("Fine-tuning complete. Model saved to:", output_dir)



